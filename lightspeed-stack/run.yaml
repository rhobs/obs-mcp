version: "2"
image_name: minimal-viable-llama-stack-configuration

apis:
  - agents
  - safety
  - vector_io
  - tool_runtime
  - inference
  - telemetry

benchmarks: []
container_image: null
datasets: []
external_providers_dir: /tmp
inference_store:
  db_path: .llama/distributions/ollama/inference_store.db
  type: sqlite
logging: null
metadata_store:
  db_path: .llama/distributions/ollama/registry.db
  namespace: null
  type: sqlite
providers:
  inference:
    - provider_id: openai
      provider_type: remote::openai
      config:
        api_key: ${env.OPENAI_API_KEY}
    # - provider_id: gemini
    #   provider_type: remote::gemini
    #   config:
    #     api_key: ${env.GOOGLE_API_KEY}
    # - provider_id: ollama
    #   provider_type: remote::ollama
    #   config:
    #     url: http://localhost:11434
    #     api_key: ignored
  agents:
    - config:
        persistence_store:
          db_path: .llama/distributions/ollama/agents_store.db
          namespace: null
          type: sqlite
        responses_store:
          db_path: .llama/distributions/ollama/responses_store.db
          type: sqlite
      provider_id: meta-reference
      provider_type: inline::meta-reference
  tool_runtime:
    - provider_id: model-context-protocol
      provider_type: remote::model-context-protocol
      config: {}
  telemetry:
    - config:
        service_name: "lightspeed-stack"
        sinks: sqlite
        sqlite_db_path: .llama/distributions/ollama/trace_store.db
      provider_id: meta-reference
      provider_type: inline::meta-reference
# server:
#   auth: null
#   host: null
#   port: 8321
#   quota: null
#   tls_cafile: null
#   tls_certfile: null
#   tls_keyfile: null
shields: []
vector_dbs: []
models:
  - model_id: gpt-4o-mini
    provider_id: openai
    model_type: llm
    provider_model_id: gpt-4o-mini
  - model_id: gpt-4o
    provider_id: openai
    model_type: llm
    provider_model_id: gpt-4o
#   - model_id: gemini/gemini-2.5-flash
#     provider_id: gemini
