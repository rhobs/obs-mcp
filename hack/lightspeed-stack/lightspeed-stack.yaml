# Configuration file for the Lightspeed Core Service (LCS)
# Taken from https://github.com/openshift/genie-web-client/blob/main/backend/lightspeed-stack/lightspeed-stack.yaml
name: Lightspeed Core Service (LCS)
service:
  host: localhost
  port: 8080
  auth_enabled: false
  workers: 1
  color_log: true
  access_log: true
llama_stack:
  use_as_library_client: true
  library_client_config_path: run.yaml
user_data_collection:
  feedback_enabled: true
  feedback_storage: "/tmp/data/feedback"
  transcripts_enabled: true
  transcripts_storage: "/tmp/data/transcripts"
authentication:
  module: "noop"
conversation_cache:
  type: "sqlite"
  sqlite:
    db_path: "/tmp/data/conversation-cache.db"
inference:
  default_model: gpt-4o-mini
  default_provider: openai
mcp_servers:
  - name: "obs"
    provider_id: "model-context-protocol"
    url: "http://localhost:9100/mcp"
  # - name: "layout-manager"
  #   provider_id: "model-context-protocol"
  #   url: "http://localhost:9081/mcp"
customization:
  system_prompt: |-
    Always use available tools.
